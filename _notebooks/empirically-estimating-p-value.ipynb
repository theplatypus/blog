{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirically estimating p-value\n",
    "\n",
    "> Using sklearn.model_selection.permutation_test_score, compute the p-value indicating if the score obtained by an instance of sklearn.dummy.DummyClassifier on the dataset sklearn.datasets.load_iris is obtained by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-value is about accepting or rejecting the following \"null hypothesis\", \n",
    "\n",
    "$ H_0 $ : there are no dependency between the target $ y $ and predictors $ X $\n",
    "\n",
    "In simple terms, it is the probability that an estimator obtained its score on a dataset by chance and not by acquiring insight from features.\n",
    "\n",
    "Let's take a look at the documentation of `sklearn.model_selection.permutation_test_score`.\n",
    "\n",
    "> Permutes targets to generate ‘randomized data’ and compute the empirical p-value against the null hypothesis that features and targets are independent.\n",
    "\n",
    "> The p-value represents the fraction of randomized data sets where the estimator performed as well or better than in the original data. A small p-value suggests that there is a real dependency between features and targets which has been used by the estimator to give good predictions. A large p-value may be due to lack of real dependency between features and targets or the estimator was not able to use the dependency to give good predictions.\n",
    "\n",
    "This function provides a way to empirically compute the p-value associated to an estimator fitting a dataset `(X, y)`, by evaluating it on `n_permutations` shuffled datasets where targets are permuted. \n",
    "\n",
    "If the null hypothesis is false, then we expect our estimator to achieve a lower score on the vast majority of these randomized datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 1.000\n",
      "Original Score: 0.333\n",
      "Permutation Scores: 0.333 +/- 0.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "\n",
    "score_dummy, permutation_scores_dummy, pvalue_dummy = permutation_test_score(dummy_clf, X, y, random_state=0)\n",
    "\n",
    "print(f\"P-value: {pvalue_dummy:.3f}\")\n",
    "\n",
    "print(f\"Original Score: {score_dummy:.3f}\")\n",
    "\n",
    "print(f\"Permutation Scores: {permutation_scores_dummy.mean():.3f} +/- {permutation_scores_dummy.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a p-value of `1.0` (the maximum possible), there are no doubt that the DummyClassifier gets its score by chance.\n",
    "\n",
    "It means that for each permutation done, it achieved a final score greater or equal than the original score. \n",
    "\n",
    "Indeed, in the `prior` default strategy of that estimator, each prediction is simply the class label having the most occurrences, hence the same $ \\hat{y} $ predictions vector, highlighted by `permutation_scores_dummy.std() == 0.0`.\n",
    "\n",
    "If we try another strategy that does not always return the same class label like `uniform`, we should observe slight variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.772\n",
      "Original Score: 0.313\n",
      "Permutation Scores: 0.339 +/- 0.038\n"
     ]
    }
   ],
   "source": [
    "score_dummy_uniform, permutation_scores_dummy_uniform, pvalue_dummy_uniform = permutation_test_score(DummyClassifier(strategy='uniform'), X, y, random_state=0)\n",
    "\n",
    "print(f\"P-value: {pvalue_dummy_uniform:.3f}\")\n",
    "\n",
    "print(f\"Original Score: {score_dummy_uniform:.3f}\")\n",
    "\n",
    "print(f\"Permutation Scores: {permutation_scores_dummy_uniform.mean():.3f} +/- {permutation_scores_dummy_uniform.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, as we usually accept the null hypothesis when $ p-value \\geq 0.05 $, we are way above this thresold, and should definitely accept it.\n",
    "\n",
    "But stricly speaking, it only tells us we accept the null hypothesis **for that estimator**. \n",
    "Like the documentation says, a high p-value indicates:\n",
    "\n",
    "- no real dependency between features and targets \n",
    "- **OR** the estimator was not able to use the dependency to give good predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Repeat this analysis using sklearn.ensemble.HistGradientBoostingClassifier instead of sklearn.dummy.DummyClassifier.\n",
    "> What can you conclude about:\n",
    "> - the existence of a significant statistical association between the iris type and the input features (petal and sepal width and length)?\n",
    "> - the ability of each kind of estimator to assess or not such a statistical association between features and target variable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.010\n",
      "Original Score: 0.947\n",
      "Permutation Scores: 0.333 +/- 0.039\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hist_clf = HistGradientBoostingClassifier().fit(X, y)\n",
    "\n",
    "score_hist, permutation_scores_hist, pvalue_hist = permutation_test_score(hist_clf, X, y, random_state=0)\n",
    "\n",
    "\n",
    "print(f\"P-value: {pvalue_hist:.3f}\")\n",
    "\n",
    "print(f\"Original Score: {score_hist:.3f}\")\n",
    "\n",
    "print(f\"Permutation Scores: {permutation_scores_hist.mean():.3f} +/- {permutation_scores_hist.std():.3f}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the tested estimator achieves a much better score `0.947` on the original dataset, and drops to the `DummyClassifier` performance on randomized datasets.\n",
    "\n",
    "The resulting p-value is thus very low, and we can safely reject the null hypothesis:\n",
    "- the iris dataset contains real dependency between features and labels\n",
    "- **AND** the classifier being able to use features to obtain good predictions\n",
    "\n",
    "Going back to the `DummyClassifier` case, we can now then assert that it was not able to effectively learn on provided observations. \n",
    "\n",
    "That is actually what we expect from dummy models, which are nonetheless very useful to act as relevant **baselines** to rank other estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is a simple way to explain what are p-values and their major importance when analyzing some data.\n",
    "\n",
    "Moreover, it is applicable to any estimator and does not require any knowledge on labels/features distributions, but can quickly become heavy to run if we decide to keep a number of permutations in the same order of magnitude than the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.9.12 (main, Apr  5 2022, 06:56:58) \n",
      "[GCC 7.5.0]\n",
      "sklearn.__version__='1.3.2'.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# execution infos\n",
    "\n",
    "import sys\n",
    "import sklearn\n",
    "\n",
    "print(\"Python version\")\n",
    "print(sys.version)\n",
    "\n",
    "print(f\"{sklearn.__version__=}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
